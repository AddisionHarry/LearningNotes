#! https://zhuanlan.zhihu.com/p/662743719
# 似然、似然函数与似然估计

## 1. 似然

似然（likelihood）与概率（probability）描述的是一个过程的两个方面：

- 似然描述的是已知随机变量的输出结果，随机过程参量的可能情况
- 概率描述的是已知随机过程参量，随机变量的输出结果的可能情况

接下来就可以引出似然函数的概念了：

> 似然函数是一种关于统计模型中的参数的函数，既然是函数那自变量就是模型可能的参数值，因变量就是参数取具体值的似然性，通俗来说就是实验结果已知的情况下，参数为某个具体值的“概率”。
> 一种方便区别是概率还是似然的方法是，根据定义，"谁谁谁的概率"中谁谁谁只能是概率空间中的事件，换句话说，我们只能说，**事件**(发生)的概率是多少多少(因为事件具有概率结构从而刻画随机性，所以才能谈概率)；而"谁谁谁的似然"中的谁谁谁只能是参数，比如说，参数等于 $\theta$ 时的似然是多少。

## 2. 似然函数

可以考虑这样一种情况，现在有一个随机变量是 X，我们知道他服从于一个固定的分布，而且这个分布中存在一个参数为 $\theta$（比如服从于零均值正态分布 $X\sim N(0,\sigma^2)$ 这里的 $\sigma^2$ 就是分布的参数 $\theta$），但是现在并不知道这个参数 $\theta$ 具体是多少，我们期望通过对于随机变量的若干次观测得到的序列 $\boldsymbol{x}=\{x_1,x_2,x_3,\cdots,x_n\}$ 得出，而似然函数描述的就是已知分布规律和随机序列 $\boldsymbol{x}$ 时参数 $\theta$ 取为某一个值的似然程度。这里给一下似然函数的公式：
$$
L(\theta|\boldsymbol{x})=P(\boldsymbol{x};\theta)\\
$$
值得注意的点就是这个等式的左右两边描述的东西其实不具有相同的含义，左边的似然函数的自变量是 $\theta$ 而右边概率函数的自变量是 $\boldsymbol{x}$，上述等式只是说明两个函数的函数值相等。从似然函数的表述来看这很像一个条件概率，描述的就是当事件 $\boldsymbol{x}$ 发生的条件下参数取定为 $\theta$ 的概率，从这个角度来看似然函数应该表述为：
$$
L(\theta|\boldsymbol{x})=\frac{P(\boldsymbol{x};\theta)}{P(\boldsymbol{x})}\\
$$
这就是条件概率的定义式，但是实际上我们发现似然函数的估计好像强制认为事件 $\boldsymbol{x}$ 发生的概率就是 1，这个假设主要来源于这样的两个方面：

1. 我们讨论似然的时候其实已经把样本情况确定下来了（很显然在计算似然的时候已经有这样的一组数据了），因此这个样本事件发生的概率就是 1；
2. 前面提到似然的概念都是横向比较的，也就是在有一组样本数据的情况下，讨论参数的似然程度，最终可以计算出**在这一组样本的情况下似然程度最高的参数**即这个任务本身只需要考虑似然程度的高度不需要考虑似然程度的绝对大小，因此就可以很自然地把这个很难计算或者只能计算先验概率的 $P(\boldsymbol{x})$ 省略——不管它是多少，对于一个固定的样本，他就是一个常数，不会影响似然函数的大小比较也就不会影响最终想要达成的目标；因此最终定义下来的**似然的概念只对样本负责，对于不同的样本讨论的参数的似然程度不具有可比性**。

通过上面的第二条理由我们可以得出**似然函数不具有归一性**的结论，似然函数乘以一个非零常数得到的仍然是一个似然函数，因为我们只关心参数变化时似然函数是变大还是变小而不关心似然函数的实际大小，所以最终定义下来的似然函数可以被写作：
$$
L(\theta|\boldsymbol{x})=P(\boldsymbol{x};\theta)\\
$$
有的时候也写作：
$$
L(\theta|\boldsymbol{x})=P(\boldsymbol{x}|\theta)\textrm{   Or   }L(\theta|\boldsymbol{x})=P_{\theta}(\boldsymbol{x})\\
$$
这些表述都是一个意思。

## 3. 极大似然估计(MLE)

极大似然估计(Maximum Likelihood Estimation, MLE)是似然函数最初也是最自然的应用。按照前面的思路可以很自然地写出极大似然估计的表达式：
$$
\theta^*=\arg\min_\theta L(\theta|\boldsymbol{x})\\
$$
比如下面的这个例子：

> 假设一枚均匀硬币投出正反面的概率都是 0.5，假设事件 H 为投出正面，事件 T 为投出反面：
> - 假设此时硬币正常，那么可以求出三次试验的结果为 HHT 的概率是 $0.5\times0.5\times0.5=0.125$
> - 假设此时硬币不知道正常与否，但是已知经过三次试验结果为 HHT，那么就可以估计出该硬币投出正面的概率 $P_H$ 为
> $$
> P_H=\arg\max_p L(p|HHT)=\arg\max_p P_p(HHT)=\arg\max_p p^2(1-p)=\frac{2}{3}\\
> $$

### 3.1 离散型随机变量的极大似然估计

已知离散型随机变量 X 的分布规律是 $P\{X=x\}=p(x;\theta)$，而通过多次独立重复试验可以得到随机序列 $\boldsymbol{x}=\{x_1,x_2,x_3,\cdots,x_n\}$，那么在参数 $\theta$ 下分布函数取到上述序列的概率为
$$
p(\boldsymbol{x}|\theta)=\prod_{i=1}^n p(x_i;\theta)\\
$$
因此似然函数可以被写作：
$$
L(\theta|\boldsymbol{x})=p(\boldsymbol{x}|\theta)=\prod_{i=1}^n p(x_i;\theta)\\
$$
要让似然取到极大值也就是让导函数为零：
$$
\frac{\textrm{d}}{\textrm{d}\theta}L(\theta|\boldsymbol{x})=\frac{\textrm{d}}{\textrm{d}\theta}\prod_{i=1}^n p(x_i;\theta)=0\\
$$
这种连乘的求导计算不是很好获得，因此考虑使用对数来简化计算：
$$
\frac{\textrm{d}}{\textrm{d}\theta}\ln L(\theta|\boldsymbol{x})=\frac{\textrm{d}}{\textrm{d}\theta}\ln\prod_{i=1}^n p(x_i;\theta)=\sum_{i=1}^n\frac{\textrm{d}}{\textrm{d}\theta}\ln p(x_i;\theta)=0\\
$$
可以看一下为什么这里可以使用对数函数的极值点来等价原函数的极值点。考虑一个任意的定义在实数 $\mathbb{R}$ 上的一元可导的正定 ($\forall x\neq0\in\mathbb{R},f(x)>0,f(0)=0$) 实函数 $f(x)$，其函数值的对数对应的新函数 $g(x)=\ln f(x)$ 的定义域与 $f(x)$ 是一样的，接下来考察两个函数的极值点之间的关系：
$$
\frac{\textrm{d}}{\textrm{d}x}g(x)=\frac{\textrm{d}}{\textrm{d}x}\ln f(x)=\frac{f'(x)}{f(x)}\\ \Rightarrow f'(x)=0\iff g'(x)=0\textrm{ Or }f(x)=0\\
$$
因此对于任意的正定函数 $f(x)$ 而言，极值点只能在其对数函数 $\ln f(x)$ 的极值点及**零点**的位置取到，因此只要函数可以取对数，那么通过求导得到的导函数零点也一定是原函数导函数的零点。

> 设 $X\sim B(1,p)$（伯努利分布），$x_1,x_2,\cdots,x_n$ 是该随机变量的一系列独立重复实验结果，那么该参数 p 的极大似然估计为：
> $$
> \begin{aligned} p^*&=\arg\max_p L(p|\boldsymbol{x})=\arg\max_p \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}\\ &=\arg\max_p p^{\sum x_i}(1-p)^{n-\sum x_i} \end{aligned}\\
> $$
> 想要计算似然函数的极大值，等价于计算似然函数对数的极大值，因此有：
> $$
> \begin{aligned} 0=\frac{\textrm{d}}{\textrm{d}p}\ln L(p|\boldsymbol{x}) &=\frac{\textrm{d}}{\textrm{d}p}\ln p^{\sum x_i}(1-p)^{n-\sum x_i}\\ &=\frac{\textrm{d}}{\textrm{d}p}\left[\left(\sum_{i=1}^n x_i\right)\ln p+\left(n-\sum_{i=1}^n x_i\right)\ln(1-p)\right]\\ &=\frac{1}{p}\sum_{i=1}^n x_i-\frac{1}{1-p}\left(n-\sum_{i=1}^n x_i\right) \end{aligned}\\
> $$
> 最终计算得到极大似然估计结果：
> $$
> \begin{aligned} &\frac{1-p^*}{p^*}=\frac{n-\displaystyle\sum_{i=1}^n x_i}{\displaystyle\sum_{i=1}^n x_i}\\ \Rightarrow&p^*=\frac{\displaystyle\sum_{i=1}^n x_i}{n}=\overline{x} \end{aligned}\\
> $$

### 3.2 连续型随机变量的极大似然估计

> 设 $X\sim N(\mu,\sigma^2)$，这里 $\mu,\sigma^2$ 为未知参数，$x_1,x_2,x_3,\cdots,x_n$ 是该随机变量的独立重复试验的结果，于是 X 的概率密度函数为：
> $$
> p(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\displaystyle\frac{(x-\mu)^2}{2\sigma^2}}\\
> $$
> 似然函数为：
> $$
> L(\mu,\sigma^2|\boldsymbol{x})=\prod_{i=1}^n p(x_i;\mu,\sigma^2)=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{-\displaystyle\frac{(x-\mu)^2}{2\sigma^2}}\\
> $$
> 于是可以对对数似然函数求导：
> $$
> \begin{aligned} \frac{\textrm{d}}{\textrm{d}\boldsymbol{\theta}}\ln L(\boldsymbol{\theta}|\boldsymbol{x})&= \frac{\textrm{d}}{\textrm{d}\boldsymbol{\theta}}\ln \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{-\displaystyle\frac{(x_i-\mu)^2}{2\sigma^2}}\\ &=\sum_{i=1}^n\frac{\textrm{d}}{\textrm{d}\boldsymbol{\theta}}\left[-\frac{(x_i-\mu)^2}{2\sigma^2}-\ln(\sqrt{2\pi}\sigma)\right]\\ &=\sum_{i=1}^n\begin{bmatrix} \displaystyle\frac{x_i-\mu}{\sigma^2} & \displaystyle\frac{(x_i-\mu)^2-\sigma^2}{\sigma ^3} \end{bmatrix}\\ &=\begin{bmatrix} \displaystyle\frac{1}{\sigma^2}(\sum_i x_i-n\mu) & \displaystyle\frac{1}{\sigma^3}(\sum_i (x_i-\mu)^2-n\sigma^2) \end{bmatrix}\\ &=\boldsymbol{0} \end{aligned}\\
> $$
> 于是可以得到极大似然估计结果为：
> $$
> \mu^*=\frac{1}{n}\sum_{i=1}^n x_i=\overline x\\ (\sigma^2)^*=\frac{1}{n}\sum_{i=1}^n (x_i-\overline{x})^2\\
> $$

## 3.3 最小二乘法与极大似然估计

最小二乘法的典型应用就是想从一堆数据点集中回归出残差平方和最小的直线（也可能是超平面）表达式，最小二乘的表述就是：
$$
\boldsymbol{b}^*=\arg\min_{}||Y-\boldsymbol{x}\boldsymbol{b}||^2\\
$$
看着有点复杂，其在二维平面内表述为：
$$
a^*,b^*=\arg\min_{a,b}\sum_{i=1}^n (y_i-ax_i-b)^2\\
$$
从这里看来最小二乘法的含义便是最小化平方和。接下来考虑他和极大似然估计之间的联系，既然是似然估计那么就需要假设随机分布，这里我们认为输出变量 Y 是满足正态分布的，而正态分布的均值与x的取值存在唯一的线性映射关系（这里认为x是确定性的普通变量而不是随机变量），也就是变量 Y 的期望满足：
$$
E(Y)=\mu(x)=ax+b\\
$$
并假设残差服从于零均值正态分布：
$$
Y-EY=\boldsymbol{\varepsilon}\sim N(0,\sigma^2)\\
$$
而正是这里残差的存在我们不能直接从观测值得到实际参数，所以才会想要使用最小二乘法来拟合直线，此时可以得到 $Y$ 的分布就是：
$$
Y=ax+b+\boldsymbol{\varepsilon}\sim N(ax+b,\sigma^2)\\
$$
接下来就可以使用极大似然估计来估计这里的 a 和 b 了，在已知样本为 $(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)$ 时可以写出似然函数为：
$$
\begin{aligned}
L(a,b,\sigma^2|\boldsymbol{x},Y)&=\prod_{i=1}^n f(y|x;a,b,\sigma^2)\\ &=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{\displaystyle-\frac{(y_i-ax_i-b)^2}{2\sigma^2}}\\ &=\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n e^{\displaystyle-\frac{\sum(y_i-ax_i-b)^2}{2\sigma^2}} \end{aligned}\\
$$
在不考虑 $\sigma^2$ 的估计的情况下可以得到 a 和 b 的极大似然估计满足：
$$
\begin{aligned} a^*,b^*&=\arg\max_{a,b}L(a,b,\sigma^2|\boldsymbol{x},Y)\\ &=\arg\max_{a,b}\ln L(a,b,\sigma^2|\boldsymbol{x},Y)\\
&=\arg\max_{a,b}\left[-n\ln(\sqrt{2\pi}\sigma)-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-ax_i-b)^2\right]\\ &=\arg\min_{a,b}\sum_{i=1}^n(y_i-ax_i-b)^2\\ \end{aligned}\\
$$
这就意味着，在线性回归中，**最小二乘法就是极大似然估计在误差服从于零均值正态分布时的特例**。

# 4. 参考链接

个人的学习顺序是：

1. [似然函数和最大似然估计](https://zhuanlan.zhihu.com/p/40236632)、[参数估计(二).最大似然估计](https://zhuanlan.zhihu.com/p/55791843)、[贝叶斯公式理解之似然函数，后验概率，先验概率](https://zhuanlan.zhihu.com/p/479174192)、[最小二乘法与最大似然估计](https://zhuanlan.zhihu.com/p/55793850)
2. [似然和似然函数详解](https://zhuanlan.zhihu.com/p/42598338)、[如何理解似然函数? - 知乎](https://www.zhihu.com/question/54082000)
3. [最大似然估计怎么理解和应用？ - 知乎](https://www.zhihu.com/question/484044429/answer/2277749758)
