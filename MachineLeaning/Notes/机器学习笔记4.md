# 机器学习吴恩达老师课堂笔记（四）
## 4. 一些特殊算法

### 4.1 推荐系统(Recommender system)
以电影推荐系统为例，每个用户都会对部分电影进行打分，推荐系统需要根据每部的电影的不同特征学习每个用户对于不同电影的喜好程度，从而预测用户对于没看过的电影的打分情况，从而推荐得分较高的电影。


首先规定一下符号：

- **r**代表用户是否对电影进行过打分，r(i,j)=1表明用户j给电影i打过分
- **y**代表用户给电影打的分数，$y^{(i,j)}=1$表明用户j给电影i打的分数（当且仅当r(i,j)=1的时候代表有效数据）
- $\boldsymbol{x}^{(i)}$代表电影i的特征变量的取值
- $m^{(j)}$代表用户j打过分的电影的数目
- n代表电影特征变量的个数
- $n_m$代表电影总数，$n_u$代表用户总数

学习的时候期望学得用户打分的表达式$(\boldsymbol{\theta}^{(j)})^T\boldsymbol{x}^{(i)}$，而优化目标就是这里的学习参数：
$$
\min_{\boldsymbol{\theta}^{(j)}}\left[\frac{1}{2m^{(j)}}\sum_{i:r(i,j)=1}\left((\boldsymbol{\theta}^{(j)})^T\boldsymbol{x}^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2m^{(j)}}\sum_{k=1}^n\left(\boldsymbol{\theta}_k^{(j)}\right)^2\right]\\
$$
实际上这是一个有监督学习过程$\boldsymbol{\theta}^{(j)},\boldsymbol{x}^{(i)}\in\mathbb{R}^{n+1}$，不过这里只是对单独的一个用户进行计算，实际上一个推荐系统需要对每个用户都单独计算代价函数：
$$
\min_{\boldsymbol{\theta}^{(1)},\boldsymbol{\theta}^{(2)},\cdots,\boldsymbol{\theta}^{(n_u)}}\left[\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left((\boldsymbol{\theta}^{(j)})^T\boldsymbol{x}^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n\left(\boldsymbol{\theta}_k^{(j)}\right)^2\right]\\
$$
不过实际上对于每一部电影，很难获得其所有特征变量，因此需要其他方法来解决这个问题。首先需要搜集一批用户对于不同电影的喜好程度（相当于人工搜集参数$\boldsymbol{\theta}$），然后根据搜集到的数据优化出电影的各特征变量的值：
$$
\min_{\boldsymbol{x}^{(1)},\boldsymbol{x}^{(2)},\cdots,\boldsymbol{x}^{(n_m)}}\left[\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}\left((\boldsymbol{\theta}^{(j)})^T\boldsymbol{x}^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n\left(\boldsymbol{x}_k^{(j)}\right)^2\right]\\
$$
在数据不稀疏的情况下，如果每个用户都对每个电影进行了评价，我们就可以利用上面的两套步骤反复迭代最终收敛出一组合适的电影特征变量取值和观众取向参数，这就是协同过滤算法。不过实际上想要实现这个目标没有必要反复迭代，只需要重新写出一个代价函数为：
$$
J(\boldsymbol{x}^{(1)},\boldsymbol{x}^{(2)},\cdots,\boldsymbol{x}^{(n_m)},\boldsymbol{\theta}^{(1)},\boldsymbol{\theta}^{(2)},\cdots,\boldsymbol{\theta}^{(n_u)})\\
=\frac{1}{2}\sum_{(i,j):r(i,j)=1}\left((\boldsymbol{\theta}^{(j)})^T\boldsymbol{x}^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n\left(\boldsymbol{\theta}_k^{(j)}\right)^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n\left(\boldsymbol{x}_k^{(j)}\right)^2\\
$$
而优化目标就是同时对整个式子两套参数进行优化：
$$
\mathop{\min_{\boldsymbol{x}^{(1)},\boldsymbol{x}^{(2)},\cdots,\boldsymbol{x}^{(n_m)}\atop\boldsymbol{\theta}^{(1)},\boldsymbol{\theta}^{(2)},\cdots,\boldsymbol{\theta}^{(n_u)}}}J(\boldsymbol{x}^{(1)},\boldsymbol{x}^{(2)},\cdots,\boldsymbol{x}^{(n_m)},\boldsymbol{\theta}^{(1)},\boldsymbol{\theta}^{(2)},\cdots,\boldsymbol{\theta}^{(n_u)})\\
$$
值得注意的就是这里将参数合并以后为了计算方便丢弃了截距项$\boldsymbol{\theta}_0$以及预设的$\boldsymbol{x}_0$，因为现在的结果可以等效被归算到特征变量中去。


完成优化以后就可以开始推荐了，我们可以估计出用户对于他还没看过的电影的预期评分，按照评分排序进行推荐，也可以找到用户刚刚给出好评的电影，找到特征向量之差的模长最短的电影（最相似的电影）进行推荐。


实际上上述算法会存在的问题就是，如果有一个用户没有对任何电影进行打分，那么按照随机初始化的思路由于该用户没有进行任何打分所以该用户不会对代价函数中的第一项产生任何贡献，反而正则化项会使得该用户的$\boldsymbol{\theta}$快速迭代为0向量，于是推荐算法无法给出任何一个推荐电影。为了解决这个问题，可以对学习得到的参数矩阵：
$$
\boldsymbol{\Theta}=\begin{bmatrix}
\left(\boldsymbol{\theta}^{(1)}\right)^T\\
\left(\boldsymbol{\theta}^{(2)}\right)^T\\
\vdots\\
\left(\boldsymbol{\theta}^{(n_m)}\right)^T\\
\end{bmatrix}\\
$$
进行均值规范化操作，也就是计算所有用户对于每部电影的均值打分（包括预测出来的分数）得到均值向量：
$$
\boldsymbol{\mu}=\begin{bmatrix}
\mu_1 \\ \mu_2\\ \vdots \\ \mu_{n_m}
\end{bmatrix}=\frac{1}{n_u}\begin{bmatrix}
\left(\boldsymbol{\theta}^{(1)}\right)^T\\
\left(\boldsymbol{\theta}^{(2)}\right)^T\\
\vdots\\
\left(\boldsymbol{\theta}^{(n_m)}\right)^T\\
\end{bmatrix}\begin{bmatrix}
1 \\ 1 \\ \vdots \\ 1
\end{bmatrix}\\
$$
然后将参数矩阵$\boldsymbol{\Theta}$的每一列都与该均值向量做差称为新的参数矩阵，使用这种方法等价于新注册的用户的默认参数矩阵都是大众的平均值，于是就可以进行算法推荐了。不过要注意的就是在计算评分的时候还是需要将这个均值向量重新加上，即最终评分为：$(\boldsymbol{\theta}^{(j)})^T\boldsymbol{x}^{(i)}+\mu_i$.
## 大规模机器学习
随着数据集体量的增大，梯度下降需要的计算量越来越大，网络的训练速度越来越慢，因此需要额外的算法来帮助加快梯度下降。
<a name="U7V28"></a>
### 随机梯度下降(Stochastic gradient descent)
梯度下降过程需要考虑代价函数对于每一个参数的偏导数，这就导致当数据体量非常大的时候，梯度的计算会变得非常困难，这里一般把这种每次梯度下降过程将所有数据样本都考虑在内的梯度下降算法称为“批量梯度下降”(Batch gradient descent)。<br />为了避免批量梯度下降导致的每次梯度计算都需要遍历整个数据集的问题，这里提出了随机梯度下降的概念，这里以线性回归为例，首先回忆一下无正则化项的线性回归的算法结构：
:::tips
每个训练周期：<br />遍历每个参数$\theta_j$（注意数据的更新是同步的）：$$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m\left(h_{\boldsymbol{\theta}}(x^{(i)})-y^{(i)}\right)x_j^{(i)}$
:::
这里会导致算法计算速度慢的就是每次计算都要反复遍历每个数据样本（如果数据量大的话就是需要反复访问硬盘）。于是考虑随机梯度下降，首先定义对于单个数据样本的损失函数：$$\textrm{cost}(\boldsymbol{\theta},(x^{(i)},y^{(i)}))=\frac{1}{2}\left(h_{\boldsymbol{\theta}}(x^{(i)}-y^{(i)})\right)^2$$此时不难发现整体代价函数可以写作：$$J(\boldsymbol{\theta})=\frac{1}{m}\sum_{i=1}^m\textrm{cost}(\boldsymbol{\theta},(x^{(i)},y^{(i)}))$$据此可以写出随机梯度下降的算法结构：
:::tips
首先将训练集数据**随机打乱**。<br />每个训练周期：<br />遍历每个数据样本i：<br />遍历每个参数$\theta_j$（注意数据的更新是同步的）：$$\theta_j:=\theta_j-\alpha\left(h_{\boldsymbol{\theta}}(x^{(i)})-y^{(i)}\right)x_j^{(i)}$
:::
这就相当于不需要反复遍历整个训练集从而带来计算时间缩短的好处。这里的做法就是对每一个数据样本进行单独的梯度下降使得每一轮内循环结束的时候都是对于当前数据点的最优。不过会带来的问题就是训练更可能会停留在局部最优解、最终结果反复震荡等等。不过相比较于批量梯度下降过程的超长计算时间而言这也是可以接受的。下图就是描述随机梯度下降和批量梯度下降的下降路径的曲线：（红色是批量梯度下降而粉色是随机梯度下降）<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2676561/1696683000239-24f4691d-c49b-4bdf-8cb2-4c679065bfa6.png#averageHue=%23f9fdfb&clientId=ud36bc801-89e1-4&from=paste&height=204&id=u6b7e2236&originHeight=474&originWidth=581&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=284010&status=done&style=none&taskId=u3fd758d6-62a4-49e7-b461-7e3f3dfcc28&title=&width=250)<br />其实随机梯度下降还会带来其他的好处，也就是外循环训练次数可以降低，所以整体而言训练时间会被大幅度缩短。<br />这里还有一个问题就是判断随机梯度下降的收敛性问题。在批量梯度下降中一般通过绘制迭代次数和总代价函数的曲线来判断，不过在随机梯度下降中中途停止随机梯度下降计算一次整体代价函数其实不太现实（速度又会被大幅度拖慢），因此这里还是考虑采用前面定义的对于单个样本的损失函数。可以考虑在每1000轮迭代结束以后绘制一下前1000轮迭代中的平均损失函数。通过观察该图可以大致看出算法是否在正常收敛。不过显而易见的就是这种计算方式会存在比较大的噪声，下面就是四种可能出现的情况：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2676561/1696686232912-9b5445a7-196a-42e8-8c38-7f05c118aab2.png#averageHue=%23fdf9f3&clientId=ud36bc801-89e1-4&from=paste&height=221&id=uc12f978d&originHeight=611&originWidth=1104&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=303685&status=done&style=none&taskId=ud8e5b9ad-5844-4c8c-b1cd-1a30530c486&title=&width=400)

- 左上图描述的是两种学习率下平均损失函数的可能表现情况，红线的学习率低于蓝线，可以发现下降速度可能更慢但是也有可能得到一个更小的平均损失函数。这来源于实际上随机梯度下降会在最优解附近震荡，而减小学习率可以减小震荡
- 右上图描述的是增大平均损失函数的平均数量以后可能的计算结果，注意这里没有改变网络的训练过程，红线相比蓝线更加平滑但是略显滞后
- 左下图描述的是一种震荡情况，从这种震荡中很难观察到算法是不是在正常梯度下降，可以考虑增大参与平均的损失函数点数，如果平均完以后是红线说明正在缓慢地进行梯度下降，而如果是粉线则说明没有在正常学习
- 右下图说明损失函数发散了，说明需要调整学习率或者查找一下程序中是存在bug

为了避免随机梯度下降最后在全局最优解附近反复正常，有一种方法就是随着训练代数的增大逐渐减小学习率，比如这样：$$\alpha=\frac{c_1}{c_2+\textrm{iterationNum}},(c_1,c_2\in\mathbb{R}_+)$$会遇到的问题就是可能需要额外调两个参数。
<a name="QW1La"></a>
### 小批量梯度下降(Mini-batch gradient descent)
到现在为止已经介绍了两种梯度下降——批量梯度下降和随机梯度下降。这两个整体而言是这样的：

- 批量梯度下降：在每一轮迭代中使用所有的训练样本点
- 随机梯度下降：在每一轮迭代中只使用一个训练样本点

而这里要提出的小批量梯度下降则介于两者之间，每一轮迭代使用b个数据样本，这里的b是一个可调参量，被称为mini-batch的大小，典型值2~100.算法流程大概就是这样的：
:::tips
每个训练周期：<br />取i=1，b+1，2b+1，3b+1，......<br />遍历每个参数$\theta_j$（注意数据的更新是同步的）：$$\theta_j:=\theta_j-\alpha\frac{1}{b}\sum_{k=i}^{i+b-1}\left(h_{\boldsymbol{\theta}}(x^{(k)})-y^{(k)}\right)x_j^{(k)}$
:::
显然小批量梯度下降的运算速度是快于批量梯度下降的，具体原因和随机梯度下降是一样的这里就不介绍了；而依赖于更多的向量计算库，小批量梯度下降的速度可能略优于随机梯度下降。唯一不足的点就是引入了一个新的超参数b。如果b选得比较好，训练的过程将会大大加快。
<a name="KJa3s"></a>
### 在线学习
在网站上，如果存在**连续的大量数据流**，就可以构建在线学习网络，每次使用输入的数据样本进行随机梯度下降**然后丢弃**，这种方法的优势就是可以根据用户喜好动态调整销售策略。比如手机销售网站就可以根据用户的搜索信息构建出特征向量$\boldsymbol{x}$（描述字符与各个手机的匹配程度等等）和点击商品情况y学习出$p(y=1|\boldsymbol{x})$函数，从而在用户进行搜索的时候就可以按照用户的输入使用学习出来的函数输出从高到低对用户进行推荐。
<a name="XpE3Y"></a>
### 映射减少(Map reduce)与并行运算
主要思路就是将求梯度中的加法运算分发到多个处理器上单独进行然后汇总，可以提高学习速度。
<a name="tHfb3"></a>
# 例子：照片OCR(Photo Optical Character Recognition)
<a name="Fz0xu"></a>
## 整体介绍
整体而言，照片OCR完成的任务主要是四步：文字检测、字符拆分、字符识别、拼写校正。<br />首先是文字检测，就是从图中框选出有文字的部分，这一部分可以使用滑动窗口法，使用不同大小的滑动窗口扫描图像，检测窗口中是否存在文字，此时就可以训练监督学习网络从图像中判断是否存在文字。<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2676561/1696692217911-78f8ba2b-d786-4ada-a68e-3e4bc23010c6.png#averageHue=%23898887&clientId=ud36bc801-89e1-4&from=paste&height=184&id=ua4fe7f50&originHeight=745&originWidth=1416&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=441837&status=done&style=none&taskId=uace66ce1-89e9-4599-82da-b1e3d0c8411&title=&width=350)<br />比如上图通过滑动窗口法得到了左下角的灰度图，该图中灰度代表了网络认为这个位置存在文字的可能性，然后通过放大算子（可能是闭运算），最后删除长款比例明显不符合要求的内容就可以了。<br />接下来是字符拆分，仍然还是采用监督学习网络来学习滑动窗口中是否存在分隔字符，可以像这样构造数据集：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2676561/1696692505876-1a3748dd-8741-488e-bd46-d06e8be15257.png#averageHue=%23f9f9f9&clientId=ud36bc801-89e1-4&from=paste&height=123&id=ue1ce3ac8&originHeight=458&originWidth=1305&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=281117&status=done&style=none&taskId=u90f4f38d-b646-47f4-a166-532219952ab&title=&width=350)<br />然后就可以通过这个网络对整块的字符串进行划分得到单个的字符，最后使用多分类的监督学习网络分别识别每个字符即可。
<a name="QxxIL"></a>
## 人工数据合成
为了获得更大体量的数据集，会使用到人工数据合成的方法，人工数据合成也分为两种——从头合成新的数据以及根据已有数据进行变形扩充得到新数据。<br />以OCR任务为例，从头生成新的数据就是可以从网上下载字体库并生成相应的图片，不过为了和真实场景下拍照得到的图片相吻合一般来说要进行一些其他变换操作，这种变换操作的逼真程度就决定了数据集的质量。<br />变形扩充主要就在已有的数据集的基础上进行一些操作变换实现扩充的效果，比如下图就实现了16倍的数据扩充：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2676561/1696693072920-8dbae1ad-44c3-400c-8bc6-b10786764a90.png#averageHue=%23bfbfbf&clientId=ud36bc801-89e1-4&from=paste&height=159&id=uc0c35b72&originHeight=602&originWidth=1133&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=468166&status=done&style=none&taskId=uae147211-d083-4993-b73a-ec251ec6c9f&title=&width=300)<br />不过这些变形操作需要注意会不会引起数据原始信号的丢失，以及加入的变形是否有意义，比如这个例子中加入高斯噪声的意义就不是很大，可以被认为是无效变换。<br />最后有几点值得讨论：

1. 在进行人工数据合成之前最好需要先观察一下学习曲线看看数据集的加入是否确实会给网络性能带来比较大的提升，否则数据合成就是无意义的。
2. 进行数据合成需要花费多少精力（人工变形扩充、数据集标注、外包）
<a name="OqgC1"></a>
## 上限分析
上限分析在优化决策中体现得尤为重要，简单来说就是测试一下系统中的每个模块的性能提升给整体系统的性能提升带来多大的改善。然后可以来讨论是否值得在某个模块上花大量时间做改进（找到主要矛盾）。<br />以这里的OCR识别为例：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2676561/1696694102048-3a4fbc71-a921-4e8c-b825-68cba5977fe2.png#averageHue=%23cbd0b4&clientId=ud36bc801-89e1-4&from=paste&height=90&id=u774784cb&originHeight=241&originWidth=1430&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=331749&status=done&style=none&taskId=uf1a493ad-ce5a-47ac-bca9-5ba25fd03f7&title=&width=534)<br />我们可以先测量在测试集下整体系统的工作准确率，然后使用人工方式完成文字检测（比如在看到一张图片以后人为框出有文字的部分，相当于模拟了一个工作准确率100%的模块）然后再测量整套系统的工作准确率，这一部分准确率的提升就是优化文字检测模块能带来的上限，依次类推。比如如果最后测量到的各种操作下的系统准确率是这样的：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2676561/1696694260762-e1234ae1-65cf-48ca-ab7b-cb218f0ea9c7.png#averageHue=%23f1f1f1&clientId=ud36bc801-89e1-4&from=paste&height=136&id=ua886d713&originHeight=310&originWidth=912&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=154656&status=done&style=none&taskId=u0ffc010e-484d-4ff1-ac98-fe410d15e7a&title=&width=400)<br />这就说明在文字分割部分花大把精力做性能提升就是没什么意义的。
